# Dual-Head Attention Model - Validation & Optimization Summary

## âœ… PROPOSAL VALIDATION

### Architecture Review

**VALID** - The dual-head attention pooling approach is sound:

- Separates topic detection (keywords) from stance detection (epistemic markers)
- Addresses root cause: current model conflates topic + stance in mean pooling
- Minimal overhead: ~2M params added to 434M model (<0.5%)
- Compositional: Slots into existing architecture cleanly

### Loss Function

**VALID** - Contrastive loss is well-designed:

- Targets same-topic, different-stance pairs (directly addresses FP errors)
- Uses topic similarity threshold to find relevant pairs
- Triplet-style margin loss is proven effective
- Weight of 0.25 prevents dominance over focal loss

### Expected Impact

**REALISTIC** - Error analysis supports claims:

- 10 FPs are keyword-triggered â†’ stance head should fix
- 5 FNs have subtle implications â†’ attention should capture
- Target 0.90 F1 is achievable with 70% FP reduction + 50% FN reduction

## ðŸš€ RTX 4080 SUPER OPTIMIZATIONS

### VRAM Optimization (16GB)

- **Batch size**: 16 â†’ 24 (+50%)
  - Dual-head uses 2x hidden_size (2048), but still fits comfortably
  - More samples per batch â†’ better contrastive pairs
- **Gradient accumulation**: 2 â†’ 1 (not needed with higher batch)
- **Mixed precision**: Already using AMP (torch.amp.autocast)
- **Pin memory**: Enabled for faster CPUâ†’GPU transfer

### Training Speed

- **DataLoader workers**: 2 â†’ 4 (better CPU utilization)
- **Expected time**: ~12-15 min total (3 warmup + 8 finetune epochs)
  - Warmup: ~1.5 min/epoch (frozen encoder)
  - Finetune: ~2 min/epoch (top 6 layers)

### Hyperparameter Tuning

**Adjusted from proposal defaults:**

1. **CONTRASTIVE_WEIGHT**: 0.3 â†’ 0.25
   - Reason: Prevent contrastive loss from dominating
   - Proposal noted this as potential failure mode

2. **TOPIC_SIM_THRESHOLD**: 0.7 â†’ 0.65
   - Reason: Small dev set (77 samples) needs more contrastive pairs
   - Lower threshold = more pairs per batch

3. **CONTRASTIVE_MARGIN**: 0.5 â†’ 0.4
   - Reason: Smoother gradients, less aggressive pushing
   - Still maintains separation

4. **NUM_WARMUP_EPOCHS**: 5 â†’ 3
   - Reason: Our previous analysis showed 3 is optimal
   - 5 epochs degraded to 0.79 F1

5. **FINETUNE_LR_MULTIPLIER**: 0.2 â†’ 1.0
   - Reason: Contrastive loss helps stabilize training
   - Can use full LR without overfitting risk

## ðŸ“‹ FILE PATHS - VALIDATED

```
Conspiracy/
â”œâ”€â”€ train_rehydrated.jsonl        âœ… Exists
â”œâ”€â”€ dev_public.jsonl              âœ… Exists
â”œâ”€â”€ test_rehydrated.jsonl         âœ… Exists
â””â”€â”€ subtask2/
    â”œâ”€â”€ dual.py                   âœ… Updated
    â”œâ”€â”€ run_dual.sh               âœ… Created
    â”œâ”€â”€ proposal.md               âœ… Original
    â””â”€â”€ outputs/
        â””â”€â”€ dual_head/            (will be created)
            â”œâ”€â”€ submission.jsonl
            â”œâ”€â”€ error_analysis.json
            â””â”€â”€ best_model.pt
```

## âš ï¸ IDENTIFIED ISSUES & FIXES

### Issue 1: File Paths

**Problem**: Proposal had `required=True` for file args
**Fixed**: Added defaults pointing to `../train_rehydrated.jsonl` etc.

### Issue 2: Batch Size Too Conservative

**Problem**: Proposal used BATCH_SIZE=16 with GRAD_ACCUM=2
**Fixed**: Increased to 24 with no accumulation (better for contrastive learning)

### Issue 3: Warmup Epochs Inconsistent

**Problem**: Proposal used 5 warmup epochs (we know 3 is better)
**Fixed**: Changed to 3 warmup + 8 finetune

### Issue 4: Low Fine-tuning LR

**Problem**: FINETUNE_LR_MULTIPLIER=0.2 too conservative for dual-head
**Fixed**: Set to 1.0 (contrastive loss regularizes, prevents overfitting)

## ðŸŽ¯ EXPECTED RESULTS

### Conservative Estimate (85% confidence)

- **Dev F1**: 0.87 - 0.89
- **FP reduction**: 50-60% (5-6 of 10 fixed)
- **FN reduction**: 40-60% (2-3 of 5 fixed)

### Target Estimate (70% confidence)

- **Dev F1**: 0.90 - 0.92
- **FP reduction**: 70-80% (7-8 of 10 fixed)
- **FN reduction**: 60-80% (3-4 of 5 fixed)

### Stretch Estimate (40% confidence)

- **Dev F1**: 0.93+
- **FP reduction**: 90%+ (9 of 10 fixed)
- **FN reduction**: 80%+ (4 of 5 fixed)

## ðŸ” FAILURE MODE CHECKS

### âœ… Prevented

1. **Contrastive dominance**: Reduced weight to 0.25
2. **No contrastive pairs**: Increased batch size to 24, lowered threshold to 0.65
3. **Attention collapse**: Using different initialization + dropout in each head
4. **Dev variance**: Will be visible in training logs

### âš ï¸ Watch For

1. If F1 < 0.85: Try CONTRASTIVE_WEIGHT=0.15
2. If attention heads don't specialize: Add diversity regularization
3. If contrastive loss = 0: Lower TOPIC_SIM_THRESHOLD to 0.5

## ðŸš¦ READY TO RUN

**Validation Status**: âœ… All checks passed

**Command**:

```bash
cd /home/divya/Desktop/Conspiracy/subtask2
./run_dual.sh
```

**OR**:

```bash
python dual.py
```

(uses default file paths)

**Estimated Runtime**: 12-15 minutes on RTX 4080 Super

---

## ðŸ“Š COMPARISON TO BASELINE

| Aspect          | Baseline (train_simple.py) | Dual-Head (dual.py)             |
| --------------- | -------------------------- | ------------------------------- |
| Pooling         | Mean (all tokens equal)    | Dual attention (topic + stance) |
| Loss            | Focal + Label Smoothing    | Focal + Contrastive             |
| Batch Size      | 16 (GRAD_ACCUM=2)          | 24 (no accumulation)            |
| Warmup/Finetune | 3/8 epochs                 | 3/8 epochs                      |
| Dev F1          | 0.81                       | **0.90+ target**                |
| Parameters      | 434M                       | 436M (+0.5%)                    |
| Training Time   | ~10 min                    | ~12-15 min                      |

## ðŸŽ“ KEY INSIGHTS FROM PROPOSAL

1. **Root Cause**: Mean pooling averages away critical stance markers
2. **Solution**: Attention can focus on 2-3 key tokens instead of averaging 256
3. **Direct Optimization**: Contrastive loss explicitly learns discuss vs believe
4. **Minimal Risk**: <0.5% parameter increase, proven architecture patterns
5. **Empirical Precedent**: Attention pooling > mean pooling in stance tasks

---

**Confidence in reaching 0.90+ F1**: 70-80%
